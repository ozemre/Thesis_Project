{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "from math import sqrt\n",
    "from math import exp\n",
    "import math\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.layers import Dense,Input, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "from keras import optimizers\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "from matplotlib import pyplot\n",
    "#from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from random import gauss\n",
    "from random import seed\n",
    "#from pandas import Series\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from matplotlib import pyplot\n",
    "#from statsmodels.graphics.tsaplots import plot_pacf\n",
    "#from statsmodels.graphics.tsaplots import plot_acf\n",
    "#from statsmodels.tsa.stattools import adfuller\n",
    "from random import randrange\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from random import gauss\n",
    "from random import seed\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "from scipy.stats.stats import pearsonr    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'04-11 22:16'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "datetime.now().strftime('%m-%d %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Experiment_Ready as experim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\murat.ozemre\\\\Desktop\\\\Thesis_Project\\\\Data_Learning_Prediction'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Product_Type_Options = [841810,841840,841850]\n",
    "Product=Product_Type_Options[2] #841810\n",
    "\n",
    "Productname=Product\n",
    "Exp_Country='TUR' # 'CHN'\n",
    "Imp_Country='GBR'\n",
    "\n",
    "if Exp_Country=='CHN':\n",
    "    Currency='CNY'\n",
    "    EXP0='TUR'   \n",
    "elif Exp_Country=='TUR':\n",
    "    Currency='TRY'\n",
    "    EXP0='CHN'   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TUR\n"
     ]
    }
   ],
   "source": [
    "#Default values\n",
    "\n",
    "dependenttype='same'\n",
    "percent=100\n",
    "MonthSeries=\"3\"\n",
    "\n",
    "if Exp_Country=='CHN':\n",
    "        if Product==841810:\n",
    "            MonthSeries=\"_34\"\n",
    "            percent=50\n",
    "            dependenttype='same'\n",
    "        elif Product==841840:\n",
    "            MonthSeries=\"_345\"\n",
    "            percent=50\n",
    "            dependenttype='log'\n",
    "        elif Product==841850:\n",
    "            MonthSeries=\"_34\"\n",
    "            percent=100\n",
    "            dependenttype='log'\n",
    "elif Exp_Country=='TUR':\n",
    "        if Product==841810:\n",
    "            MonthSeries=\"_67\"\n",
    "            percent=50\n",
    "            dependenttype='same'\n",
    "        elif Product==841840:\n",
    "            MonthSeries=\"4\"\n",
    "            percent=50\n",
    "            dependenttype='same'\n",
    "        elif Product==841850:\n",
    "            MonthSeries=\"_45\"\n",
    "            percent=50\n",
    "            dependenttype='same'\n",
    "\n",
    "print(Exp_Country)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('..')\n",
    "os.chdir('Data_Sources_and_Preparation')\n",
    "\n",
    "Data_Core1 = pd.ExcelFile('Data_{}_{}_{}.xlsx'.format(Exp_Country,Imp_Country,Product))\n",
    "y = Data_Core1.parse('Y', header=0,index_col=None, na_values=['NA'])['{}_{}_{}'.format(Exp_Country,Imp_Country,Product)]\n",
    "Z = Data_Core1.parse('Y', header=0,index_col=None, na_values=['NA'])[['Date','Year','Month']]\n",
    "\n",
    "X = Data_Core1.parse('X{}'.format(MonthSeries), header=0,index_col=None, na_values=['NA'])\n",
    "X.drop(['Date','Year','Month'], axis=1, inplace=True)\n",
    "X_Column_Names=list(X.columns.values)\n",
    "n_feature=X.shape[1]\n",
    "\n",
    "#y=y.iloc[3:]\n",
    "#X=X.iloc[3:,:]\n",
    "#Z=Z.iloc[3:,:]\n",
    "\n",
    "Xhat = Data_Core1.parse('Xhat', header=0,index_col=None, na_values=['NA'])\n",
    "Xhat.drop(['Date','Year','Month'], axis=1, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if dependenttype=='same':\n",
    "    dependent_variable=y\n",
    "elif dependenttype=='log':\n",
    "    dependent_variable=y.apply(np.log)\n",
    "elif dependenttype=='sqrt':\n",
    "    dependent_variable=y.apply(np.sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate Feature Importace according Month Series\n",
    "number_of_split_for_feature=100\n",
    "result=experim.get_feature_importance_result (X,dependent_variable,Z,n_feature,number_of_split_for_feature)\n",
    "scored_feature_indices=result[0]\n",
    "feature_score=result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Omit Features according percentage\n",
    "percentile=100-percent\n",
    "threshold_for_feature_selection=np.percentile(feature_score[scored_feature_indices], percentile)\n",
    "X_threshold=X.iloc[:,scored_feature_indices[feature_score[scored_feature_indices]>threshold_for_feature_selection]]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  3, 14, 15, 13, 41, 10, 38,  8, 17, 36, 47, 37, 31, 43,  7, 32,\n",
       "        5, 39, 42, 35, 44,  4, 16, 27, 29, 33,  9, 19, 11, 40,  2,  0, 45,\n",
       "       28, 30,  6, 21, 54, 26, 34, 50, 12, 49, 55, 22, 46, 48, 18, 20, 51,\n",
       "       52, 25, 53, 23, 24], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scored_feature_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set X train, X test, y train, y test\n",
    "rs=42 # 841810 -3\n",
    "\n",
    "Scaled_Train_Test_Split=experim.X_Y_scaler_train_test_Split(X_threshold,dependent_variable,Z,random=rs)\n",
    "\n",
    "X_train = Scaled_Train_Test_Split[0].values\n",
    "X_test = Scaled_Train_Test_Split[1].values\n",
    "y_train = Scaled_Train_Test_Split[2].values\n",
    "y_test = Scaled_Train_Test_Split[3].values\n",
    "scaler_X = Scaled_Train_Test_Split[4]  \n",
    "scaler_y = Scaled_Train_Test_Split[5]\n",
    "scaled_value_X=Scaled_Train_Test_Split[6]\n",
    "scaled_value_y=Scaled_Train_Test_Split[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randomforest rs= 76\n",
      "randomforest rs= 47\n",
      "randomforest rs= 67\n",
      "randomforest rs= 43\n",
      "randomforest rs= 6\n",
      "randomforest rs= 37\n",
      "randomforest rs= 2\n",
      "randomforest rs= 75\n",
      "randomforest rs= 82\n",
      "randomforest rs= 84\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.614, 0.762, 0.732, 0.791, 0.699, 0.761, 0.666, 0.743, 0.646, 0.675],\n",
       " [0.614, 0.762, 0.732, 0.791, 0.699, 0.761, 0.666, 0.743, 0.646, 0.675])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experim.experiment_RandomForest(10,\n",
    "                  X_train, X_test, y_train, y_test,scaler_y,\n",
    "                                rand=20,is_random_fixed='FALSE',dependenttype=dependenttype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randomforest rs= 43\n",
      "randomforest rs= 43\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.84, 0.84], [0.84, 0.84])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TUR 841840 RS =43\n",
    "experim.experiment_RandomForest(2,\n",
    "                  X_train, X_test, y_train, y_test,scaler_y,est=3000,rand=43,dependenttype=dependenttype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM EXPERIMENT (with fixed split set) SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('..')\n",
    "os.chdir('Data_Learning_Prediction/Plots_for_LSTM')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LSTM EXPERIMENT\n",
    "\n",
    "#def experiment(repeats,n_epochs,n_neurons,learning_rate,bs,rs,X,y,date):\n",
    "def experiment_LSTM(repeats,n_epochs,n_neurons,learning_rate,bs,drop_rate,do_batch,do_model,train_X, test_X, train_y, test_y,scaler_x,scaler_y):\n",
    "    K.clear_session()\n",
    "\n",
    "#    print(type(train_X))\n",
    "    train_X =train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "\n",
    "#    train_X =train_X.reshape((train_X.shape[0], train_X.shape[1],1))\n",
    "#    test_X = test_X.reshape((test_X.shape[0], test_X.shape[1],1))\n",
    "#    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    " \n",
    "    \n",
    "    error_rmse = list()\n",
    "    error_r2hat = list()\n",
    "    error_r2 = list()\n",
    "    for r in range(repeats):\n",
    "        \n",
    "        print('Shape of X Train',train_X.shape[1],train_X.shape[2])\n",
    "    \n",
    "\n",
    "        if do_model=='A':\n",
    "            model = Sequential() \n",
    "            model.add(LSTM(n_neurons,input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "            model.add(Dropout(drop_rate))\n",
    "            \n",
    "            model.add(Dense(n_neurons))\n",
    "            model.add(Dropout(drop_rate))\n",
    "            \n",
    "            model.add(Dense(1))\n",
    "#            model.add(Activation('sigmoid'))\n",
    "            model.add(Activation('linear'))\n",
    "        \n",
    "        elif do_model=='B':   \n",
    "            input_layer=Input(shape=(train_X.shape[1], train_X.shape[2]),dtype='float32')\n",
    "            lstm_layer1=LSTM(n_neurons,input_shape=(train_X.shape[1],train_X.shape[2]),\n",
    "                         dropout=drop_rate, \n",
    "                         recurrent_dropout=drop_rate,\n",
    "                         return_sequences=True)(input_layer)\n",
    "            lstm_layer2=LSTM(n_neurons,input_shape=(train_X.shape[1],n_neurons),\n",
    "                         dropout=drop_rate, \n",
    "                         recurrent_dropout=drop_rate,\n",
    "                         return_sequences=False)(lstm_layer1)\n",
    "            dropout_layer=Dropout(drop_rate)(lstm_layer2)\n",
    "\n",
    "            output_layer=Dense(1,activation=\"linear\")(dropout_layer)\n",
    "#            output_layer=Dense(1,activation=\"linear\")(lstm_layer2)\n",
    "         \n",
    "\n",
    "        #ix layerlarda Activation için RELU Output için linear uygun oluyor. Kaynak Siraj Raval\n",
    "        \n",
    "            model=Model(inputs=input_layer, outputs=output_layer)\n",
    "        \n",
    "        \n",
    "        #decay_rate = learning_rate / n_epochs\n",
    "        \n",
    "        decay_rate = 0.8\n",
    "        momentum = 0.9\n",
    "         \n",
    "        sgd = optimizers.SGD(lr=learning_rate, clipvalue=0.3,momentum=momentum, decay=decay_rate,nesterov=True)\n",
    "        adam = optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=decay_rate, amsgrad=False)\n",
    "\n",
    "        #model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        \n",
    "        \n",
    "#        model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])\n",
    "        model.compile(loss='mean_squared_error', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "#        model=load_model(os.path.join('Data','train_dataset{}-{}neurons.hdf5'.format(Productname,n_neurons)))\n",
    "#        model=load_model(os.path.join('Data','train_dataset{}-{}neurons.hdf5'.format(Productname,n_neurons)))\n",
    "\n",
    "        model.summary()\n",
    "        \n",
    "        save_weights_at=os.path.join('Data','train_dataset{}-{}neurons.hdf5'.format(Productname,n_neurons))\n",
    "\n",
    "        save_best=ModelCheckpoint(save_weights_at, monitor='val_loss', verbose=0,\n",
    "                                 save_best_only=True, save_weights_only=False, mode='min',\n",
    "                                 period=1)\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.00001)\n",
    "        early_stop = EarlyStopping(monitor='loss', patience=10, verbose=1)\n",
    "        \n",
    "        history = model.fit(train_X, train_y, epochs=n_epochs, batch_size=bs, \n",
    "\n",
    "                            validation_data=(test_X, test_y), verbose=1, \n",
    "#                            callbacks=[reduce_lr],\n",
    "                           # callbacks=[save_best],\n",
    "\n",
    "                           # callbacks=[early_stop],\n",
    "                            shuffle=False)\n",
    "\n",
    "#        best_model=load_model(os.path.join('Data','train_dataset.hdf5')\n",
    "#        model=load_model(os.path.join('Data','train_dataset{}-{}neurons.hdf5'.format(Productname,n_neurons)))\n",
    "\n",
    "        #model=best_model\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        # summarize history for loss\n",
    "        plt.gcf().clear()\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        # summarize history for accuracy\n",
    "#        plt.plot(history.history['acc'])\n",
    "#        plt.plot(history.history['val_acc'])\n",
    "#        plt.title('model accuracy')\n",
    "#        plt.ylabel('accuracy')\n",
    "#        plt.xlabel('epoch')\n",
    "#        plt.legend(['train', 'test'], loc='upper left')\n",
    "#        plt.show()\n",
    "\n",
    "\n",
    "#        test_X_reshaped = test_X.reshape((test_X.shape[0], test_X.shape[1]))\n",
    "\n",
    "        test_X_reshaped = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "        inv_x_test = scaler_x.inverse_transform(test_X_reshaped)\n",
    "        inv_x_test = pd.DataFrame(data=inv_x_test[:,:])\n",
    "\n",
    "        # make a prediction\n",
    "        y_predict_test = model.predict(test_X)\n",
    "        y_predict_train = model.predict(train_X)\n",
    "\n",
    "        # invert scaling for forecast\n",
    "        inv_y_predict_test = scaler_y.inverse_transform(y_predict_test)\n",
    "        inv_y_predict_test = inv_y_predict_test[:,0]\n",
    "        inv_y_predict_train = scaler_y.inverse_transform(y_predict_train)\n",
    "        inv_y_predict_train = inv_y_predict_train[:,0]\n",
    "\n",
    "        # invert scaling for actual\n",
    "        y_test = test_y.reshape((len(test_y), 1))\n",
    "        inv_y_test = scaler_y.inverse_transform(test_y)\n",
    "        inv_y_test = inv_y_test[:,0]\n",
    "\n",
    "        y_train = train_y.reshape((len(train_y), 1))\n",
    "        inv_y_train = scaler_y.inverse_transform(train_y)\n",
    "        inv_y_train = inv_y_train[:,0]\n",
    "\n",
    "        # calculate RMSE for DIFFERENCE\n",
    "        rmse_test = sqrt(mean_squared_error(inv_y_test, inv_y_predict_test))\n",
    "        print('Test RMSE: %.3f' % rmse_test)\n",
    "        R2_test=int(1000*(metrics.r2_score(inv_y_test, inv_y_predict_test)))/1000\n",
    "        print('R2_test: %.3f' % R2_test)\n",
    "\n",
    "        rmse_train = sqrt(mean_squared_error(inv_y_train, inv_y_predict_train))\n",
    "        print('Train RMSE: %.3f' % rmse_train)\n",
    "        R2_train=int(1000*(metrics.r2_score(inv_y_train, inv_y_predict_train)))/1000\n",
    "        print('R2_train: %.3f' % R2_train)\n",
    "\n",
    "        # calculate RMSE for REAL VALUE\n",
    "\n",
    "        real_y_test= inv_x_test.iloc[:,0]+inv_y_test\n",
    "        real_y_predict_test=inv_x_test.iloc[:,0]+inv_y_predict_test\n",
    "        \n",
    "        real_rmse_test = sqrt(mean_squared_error(real_y_test, real_y_predict_test))\n",
    "        print('Test RMSE: %.3f' % real_rmse_test)\n",
    "        real_R2_test=int(1000*(metrics.r2_score(real_y_test, real_y_predict_test)))/1000\n",
    "        print('R2_test: %.3f' % real_R2_test)\n",
    "        \n",
    "        error_rmse.append(real_rmse_test)\n",
    "        error_r2hat.append(real_R2_test)\n",
    "        error_r2.append(R2_test)\n",
    "        \n",
    "        \n",
    "        plt.gcf().clear()\n",
    "        plt.figure(figsize=(5.5, 5.5))\n",
    "        plt.plot(range(len(inv_y_test)), inv_y_test, linestyle='-', marker='*', color='r')\n",
    "        plt.plot(range(len(inv_y_predict_test)), inv_y_predict_test, linestyle='-', marker='.', color='b')\n",
    "        plt.legend(['Actual','Predicted'], loc=2)\n",
    "        plt.title('Actual vs Predicted for {}'.format(Productname))\n",
    "        plt.ylabel('Trade Value')\n",
    "        plt.xlabel('Index')\n",
    "        plt.savefig('LSTM-LinePlt{} ,{} epochs,{} neurons,{} learning_rate,{} batch size, {} random, {} R2.png'.format(Productname,\n",
    "                                n_epochs,n_neurons,learning_rate,bs,rs,R2_test), format='png', dpi=300)\n",
    "       \n",
    "\n",
    "        \n",
    "        \n",
    "    return error_rmse,error_r2,error_r2hat,train_y,history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(115, 1, 28) (115, 1) (29, 1, 28) (29, 1)\n",
      "Shape of X Train 1 28\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1, 28)             0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1, 300)            394800    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 1,116,301\n",
      "Trainable params: 1,116,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 115 samples, validate on 29 samples\n",
      "Epoch 1/200\n",
      "115/115 [==============================] - 7s 59ms/step - loss: 0.0439 - acc: 0.0087 - val_loss: 0.0365 - val_acc: 0.0000e+00\n",
      "Epoch 2/200\n",
      " 24/115 [=====>........................] - ETA: 3s - loss: 0.0244 - acc: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-fbfc7927c273>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mdeney\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexperiment_LSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepeats\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdrop_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdo_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdo_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscaler_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscaler_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0merror_rmse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeney\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-7ff95fbc7a4e>\u001b[0m in \u001b[0;36mexperiment_LSTM\u001b[1;34m(repeats, n_epochs, n_neurons, learning_rate, bs, drop_rate, do_batch, do_model, train_X, test_X, train_y, test_y, scaler_x, scaler_y)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m                            \u001b[1;31m# callbacks=[early_stop],\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m                             shuffle=False)\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;31m#        best_model=load_model(os.path.join('Data','train_dataset.hdf5')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\Tensor_1\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\Tensor_1\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\Tensor_1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\Tensor_1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#rs=29\n",
    "rs=42\n",
    "repeats = 1\n",
    "drop_rate=0.0\n",
    "\n",
    "do_batch='TRUE'\n",
    "do_model='B'\n",
    "random_split='TRUE'\n",
    "\n",
    "e=5\n",
    "n=300\n",
    "lr=0.01\n",
    "b=1\n",
    "\n",
    "deney=experiment_LSTM(repeats,e,n,lr,b,drop_rate,do_batch,do_model,X_train, X_test, y_train, y_test,scaler_X,scaler_y)\n",
    "\n",
    "error_rmse=deney[0] \n",
    "error_r2=deney[1] \n",
    "error_r2hat=deney[2] \n",
    "train_y=deney[3]\n",
    "history=deney[4]\n",
    "print(error_r2)\n",
    "print(error_r2hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
