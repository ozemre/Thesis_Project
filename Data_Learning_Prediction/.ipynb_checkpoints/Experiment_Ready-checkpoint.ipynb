{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'02-10 18:43'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import statsmodels\n",
    "from math import sqrt\n",
    "from math import log\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from datetime import datetime\n",
    "\n",
    "import random\n",
    "from random import gauss\n",
    "from random import seed\n",
    "datetime.now().strftime('%m-%d %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Xscaler(X,y,scalertype):\n",
    "\n",
    "    if scalertype==\"Normalizer\":\n",
    "        X=pd.DataFrame(Normalizer().fit_transform(X,y))\n",
    "        print(\"normalize\")\n",
    "    elif scalertype==\"MinMaxScaler\":\n",
    "        X=pd.DataFrame(MinMaxScaler().fit_transform(X,y))\n",
    "        print(\"minmax\")\n",
    "    elif scalertype==\"MaxAbsScaler\":\n",
    "        X=pd.DataFrame(MaxAbsScaler().fit_transform(X,y))\n",
    "        print(\"maxabs\")\n",
    "    elif scalertype==\"RobustScaler\":\n",
    "        X=pd.DataFrame(RobustScaler().fit_transform(X,y))\n",
    "    elif scalertype==\"StandardScaler\":\n",
    "        X=pd.DataFrame(StandardScaler().fit_transform(X,y))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def X_Y_scaler_train_test_Split(X,y,Z,random=42):\n",
    "\n",
    "    values = X.values\n",
    "    values = values.astype('float32')\n",
    "    X_Column_X_Column_Names=X.columns\n",
    "    \n",
    "    scaler_X= MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    scaled_value_X = scaler_X.fit_transform(values)\n",
    "    scaled_value_X = pd.DataFrame(data=scaled_value_X[:,:])\n",
    "    scaled_value_X.columns=X_Column_X_Column_Names\n",
    "    \n",
    "    values = y.values\n",
    "    values = values.astype('float32')\n",
    "    values = values.reshape(-1, 1)\n",
    "    \n",
    "    scaler_y= MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    scaled_value_y = scaler_y.fit_transform(values)\n",
    "    scaled_value_y = pd.DataFrame(data=scaled_value_y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_value_X,scaled_value_y,test_size=0.2,random_state=42,stratify=Z['Month'])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, scaler_X, scaler_y, scaled_value_X, scaled_value_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For Random Forest with variable tuning \n",
    "\n",
    "def randomforest(X_train, X_test, y_train, y_test,scaler_y,rand=50,is_random_fixed='TRUE',est=10,min_leaf=1,feat='auto',max_leaf=None,min_weight=0.0,min_impurity=1e-07):\n",
    "    from sklearn.model_selection import cross_val_score   \n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    \n",
    "    if is_random_fixed == 'TRUE': \n",
    "        rs=rand\n",
    "    else :\n",
    "        rs=random.randint(1,100)\n",
    "    print('randomforest rs=',rs)\n",
    "    rfc=RandomForestRegressor(n_estimators=est,\n",
    "                              min_samples_leaf=min_leaf,\n",
    "                              random_state =rs,\n",
    "                              max_features=feat,\n",
    "                              max_leaf_nodes=max_leaf,\n",
    "                              min_weight_fraction_leaf =min_weight,\n",
    "                              min_impurity_decrease=min_impurity\n",
    "                            )\n",
    "   \n",
    "    RandomForestRegressor.fit(rfc,X_train,y_train)\n",
    "    \n",
    "    y_predict_test = rfc.predict(X_test)\n",
    "    y_predict_train = rfc.predict(X_train)\n",
    "    \n",
    "    result_test=inverse_scale_and_graph_Y_predict_and_test(y_predict_test,y_test,scaler_y,'NO')\n",
    "    result_train=inverse_scale_and_graph_Y_predict_and_test(y_predict_train,y_train,scaler_y,'NO')\n",
    "    \n",
    "    \n",
    "  \n",
    "    return result_test, result_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inverse_scale_and_graph_Y_predict_and_test(y_predict_test,y_test,scaler_y,plot_on):\n",
    "\n",
    "    y_predict_test=y_predict_test.reshape(-1, 1)\n",
    "    inv_y_predict_test = scaler_y.inverse_transform(y_predict_test)\n",
    "    predictions=inv_y_predict_test\n",
    "\n",
    "  \n",
    "    inv_y_test = scaler_y.inverse_transform(y_test)\n",
    "    inv_y_test = inv_y_test[:,0]\n",
    "\n",
    "    MAE=int(metrics.mean_absolute_error(inv_y_test, predictions))\n",
    "    MSE=int(sqrt(metrics.mean_squared_error(inv_y_test, predictions)))\n",
    "    flatten=predictions.flatten()\n",
    "    R2=int(1000*pearsonr(inv_y_test,flatten )[0]**2)/1000\n",
    "#    R2=int(1000*(metrics.r2_score(inv_y_test, predictions)))/1000\n",
    "    \n",
    "    \n",
    "    if plot_on =='YES':\n",
    "        plt.scatter(inv_y_test,predictions)\n",
    "    \n",
    "    return MAE,MSE,R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def experiment_RandomForest(repeats,\n",
    "                  X_train, X_test, y_train, y_test,scaler_y,\n",
    "                  rand=50,is_random_fixed='TRUE',\n",
    "                  est=10,min_leaf=1,feat='auto',max_leaf=None,min_weight=0.0,min_impurity=1e-07):\n",
    "    \n",
    "    error_rmse = list()\n",
    "    error_R2 = list()\n",
    "    \n",
    "    for r in range(repeats):\n",
    "\n",
    "        result=randomforest(X_train, X_test, y_train, y_test,scaler_y,\n",
    "                            rand=rand,is_random_fixed=is_random_fixed,\n",
    "                            est=est,min_leaf=min_leaf,feat=feat,max_leaf=max_leaf,\n",
    "                            min_weight=min_weight,min_impurity=min_impurity)\n",
    "\n",
    "    \n",
    "      \n",
    "        rmse_test=result[0][1]\n",
    "        R2_test=result[0][2]\n",
    "        \n",
    "        rmse_train=result[1][0]\n",
    "        R2_train=result[1][1]\n",
    "        \n",
    "        error_rmse.append(rmse_test)\n",
    "        error_R2.append(R2_test)\n",
    "    \n",
    "    return error_rmse, error_R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NeuralNetwork(X_train, X_test, y_train, y_test,scaler_y,\n",
    "                  rand=50,is_random_fixed='TRUE',\n",
    "                  activ='relu', alph=0.0001, slv='adam', max_iteration=200,  hidden_layer=(30,30)):\n",
    "    \n",
    "    \n",
    "    if is_random_fixed == 'TRUE': \n",
    "        rs=rand\n",
    "    else :\n",
    "        rs=random.randint(1,100)\n",
    "    print('neuralnetwork rs=',rs)   \n",
    "\n",
    "    MLP = MLPRegressor(\n",
    "                            activation=activ,\n",
    "                            random_state =rs,                      \n",
    "                            alpha = alph,\n",
    "                            solver=slv ,\n",
    "                            max_iter=max_iteration,  \n",
    "                            hidden_layer_sizes=hidden_layer\n",
    "                        )\n",
    "\n",
    "\n",
    "    MLPRegressor.fit(MLP,X_train,y_train)\n",
    "    \n",
    "    y_predict_test = MLP.predict(X_test)\n",
    "    y_predict_train = MLP.predict(X_train)\n",
    "    \n",
    "    result_test=inverse_scale_and_graph_Y_predict_and_test(y_predict_test,y_test,scaler_y,'NO')\n",
    "    result_train=inverse_scale_and_graph_Y_predict_and_test(y_predict_train,y_train,scaler_y,'NO')\n",
    "   \n",
    "    return result_test, result_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def experiment_NN(repeats,\n",
    "                  X_train, X_test, y_train, y_test,scaler_y,\n",
    "                  rand=50,is_random_fixed='TRUE',\n",
    "                  activ='relu',alph=0.0001, max_iteration=200, slv='adam',  hidden_layer=(30,30)):\n",
    "\n",
    "\n",
    "    error_rmse = list()\n",
    "    error_R2 = list()\n",
    "    \n",
    "    for r in range(repeats):\n",
    "            \n",
    "        result = NeuralNetwork(X_train, X_test, y_train, y_test,scaler_y,\n",
    "                               rand=rand,is_random_fixed=is_random_fixed,\n",
    "                               activ=activ,alph=alph,max_iteration=max_iteration, slv=slv, hidden_layer=hidden_layer)\n",
    "        \n",
    "        \n",
    "        rmse_test=result[0][1]\n",
    "        R2_test=result[0][2]\n",
    "        \n",
    "        rmse_train=result[1][0]\n",
    "        R2_train=result[1][1]\n",
    "        \n",
    "        error_rmse.append(rmse_test)\n",
    "        error_R2.append(R2_test)\n",
    "    \n",
    "    return error_rmse, error_R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for Feature Selection for Ver#3 End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function #1 for #3 Version for Train Test Split and Feature selection \n",
    "# Calculates Importances and R2 scores  only for each iteration\n",
    "def get_feature_importance_and_R2 (X,y,Z,n_feature,split=5):\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    rf = RandomForestRegressor()\n",
    "    number_of_split=split\n",
    "    random_state_options=np.random.randint(1,100,size=number_of_split)\n",
    "\n",
    "    feature_indices = np.ones((n_feature, number_of_split))\n",
    "    feature_importances=np.ones((n_feature, number_of_split))\n",
    "\n",
    "    scores = defaultdict(list)\n",
    "    feature_std = np.ones((n_feature, number_of_split))\n",
    "    feature_score=np.zeros((n_feature))\n",
    "\n",
    "    R2=np.ones(number_of_split)\n",
    "\n",
    "    for turn in range(number_of_split):\n",
    "        random=random_state_options[turn]\n",
    "    \n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X,y,test_size=0.2,random_state=random,stratify=Z['Month'])\n",
    "\n",
    "        RandomForestRegressor.fit(rf,X_train, Y_train)    \n",
    "   \n",
    "        flatten=rf.predict(X_test).flatten()\n",
    "        R2[turn]=int(1000*pearsonr(Y_test,flatten )[0]**2)/1000\n",
    " \n",
    "        importances = rf.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "\n",
    "        feature_importances[:,turn]=importances\n",
    "        feature_indices[:,turn]=indices\n",
    "\n",
    "    return R2,feature_importances,feature_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function #2 for #3 Version for Train Test Split and Feature selection \n",
    "# For each Iteration combinesimportances and R2 scores get 1 end result\n",
    "def combine_feature_importance_and_R2 (score_coefficient,feature_importances,n_feature,number_of_split):\n",
    "    feature_score=np.zeros((n_feature))\n",
    "    for i in range(n_feature):\n",
    "        feature_score[i]=0\n",
    "    \n",
    "        for j in range(number_of_split):\n",
    "                \n",
    "            importances_coeff=int((feature_importances[i,j]*10000))/10000\n",
    "        \n",
    "            score_coeff=int((score_coefficient[j]+1)*10)/10\n",
    "\n",
    "            score=score_coeff*(importances_coeff)\n",
    "\n",
    "            feature_score[i]=feature_score[i]+score\n",
    "    \n",
    "    scored_feature_indices = (np.argsort(feature_score)[::-1])\n",
    "\n",
    "    return scored_feature_indices,feature_score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function #3 for #3 Version for Train Test Split and Feature selection \n",
    "# This Function first calls Function #1 and then Function #2 \n",
    "def get_feature_importance_result (X,y,Z,n_feature,number_of_split):\n",
    "    \n",
    "    result=get_feature_importance_and_R2 (X,y,Z,n_feature,number_of_split)\n",
    "    R2=result[0]\n",
    "    feature_importances=result[1]\n",
    "    feature_indices=result[2]\n",
    "\n",
    "    R2_Adj=1-R2\n",
    "    score_coefficient=n_feature*(R2_Adj - np.max(R2_Adj))/-np.ptp(R2_Adj)\n",
    "\n",
    "    result=combine_feature_importance_and_R2(score_coefficient,feature_importances,n_feature,number_of_split)\n",
    "    scored_feature_indices=result[0]\n",
    "    feature_score=result[1]/number_of_split\n",
    "\n",
    "    return scored_feature_indices,feature_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for Feature Selection for Ver#3 Start\n",
    "3 Different Functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
